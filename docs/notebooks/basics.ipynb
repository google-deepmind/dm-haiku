{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_"
      },
      "source": [
        "# Haiku Basics\n",
        "\n",
        "In this Colab, you will learn the basics of Haiku.\n",
        "\n",
        "**What and Why ?**\n",
        "\n",
        "[Haiku](https://github.com/deepmind/dm-haiku) is a simple neural network library for JAX that enables users to use familiar object-oriented programming models while allowing full access to JAX's pure function transformations. Haiku is designed to make the common things we do such as managing model parameters and other model state simpler and similar in spirit to the [Sonnet](https://github.com/deepmind/sonnet) library that has been widely used across DeepMind. It preserves Sonnet's module-based programming model for state management while retaining access to JAX's function transformations. Haiku can be expected to compose with other libraries and work well with the rest of JAX.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_"
      },
      "outputs": [],
      "source": [
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_"
      },
      "source": [
        "### A first example with hk.transform\n",
        "\n",
        "As an initial introduction to Haiku, let us construct a linear module with weights and biases with custom initializations.\n",
        "\n",
        "Similar to Sonnet modules, Haiku modules are Python objects that hold references to their own parameters, other modules, and methods that apply functions on user inputs. On the other hand, since JAX operates on pure function transformations, Haiku modules cannot be instantiated verbatim. Rather, the modules need to be wrapped into pure function transformations. \n",
        "\n",
        "Haiku provides a simple function transformation, `hk.transform`, that turns functions that use these object-oriented, functionally \"impure\" modules into pure functions that can be used with JAX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_"
      },
      "outputs": [],
      "source": [
        "class MyLinear1(hk.Module):\n",
        "\n",
        "  def __init__(self, output_size, name=None):\n",
        "    super().__init__(name=name)\n",
        "    self.output_size = output_size\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    j, k = x.shape[-1], self.output_size\n",
        "    w_init = hk.initializers.TruncatedNormal(1. / np.sqrt(j))\n",
        "    w = hk.get_parameter(\"w\", shape=[j, k], dtype=x.dtype, init=w_init)\n",
        "    b = hk.get_parameter(\"b\", shape=[k], dtype=x.dtype, init=jnp.ones)\n",
        "    return jnp.dot(x, w) + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_"
      },
      "outputs": [],
      "source": [
        "def _forward_fn_linear1(x):\n",
        "  module = MyLinear1(output_size=2)\n",
        "  return module(x)\n",
        "\n",
        "forward_linear1 = hk.transform(_forward_fn_linear1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_"
      },
      "source": [
        "We see that the forward wrapper object now contains two methods, `init` and `apply`, that are used to initialize the variables and do forward inference on the module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transformed(init=<function without_state.<locals>.init_fn at 0x7fa22fe754c0>, apply=<function without_state.<locals>.apply_fn at 0x7fa22fe75550>)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "forward_linear1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_"
      },
      "source": [
        "Calling the init method will initialize the parameters of the network and return them, as can be seen below. The init method takes a `jax.random.PRNGKey` and a sample input (usually just some dummy values to tell the networks about the expected shapes).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/haiku-docs-env/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
            "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FlatMapping({\n",
            "  'my_linear1': FlatMapping({\n",
            "                  'w': DeviceArray([[-0.30350363,  0.5123802 ],\n",
            "                                    [ 0.08009142, -0.3163005 ],\n",
            "                                    [ 0.6056666 ,  0.5820702 ]], dtype=float32),\n",
            "                  'b': DeviceArray([1., 1.], dtype=float32),\n",
            "                }),\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "dummy_x = jnp.array([[1., 2., 3.]])\n",
        "rng_key = jax.random.PRNGKey(42)\n",
        "\n",
        "params = forward_linear1.init(rng=rng_key, x=dummy_x)\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_"
      },
      "source": [
        "We can now use the params to apply the forward function to some inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output 1 : [[2.6736789 2.6259897]]\n",
            "Output 2 (same as output 1): [[2.6736789 2.6259897]]\n",
            "Output 3 : [[3.820442 4.960439]\n",
            " [4.967205 7.294889]]\n"
          ]
        }
      ],
      "source": [
        "sample_x = jnp.array([[1., 2., 3.]])\n",
        "sample_x_2 = jnp.array([[4., 5., 6.], [7., 8., 9.]])\n",
        "\n",
        "output_1 = forward_linear1.apply(params=params, x=sample_x, rng=rng_key)\n",
        "# Outputs are identical for given inputs since the forward inference is non-stochastic.\n",
        "output_2 = forward_linear1.apply(params=params, x=sample_x, rng=rng_key)\n",
        "\n",
        "output_3 = forward_linear1.apply(params=params, x=sample_x_2, rng=rng_key)\n",
        "\n",
        "print(f'Output 1 : {output_1}')\n",
        "print(f'Output 2 (same as output 1): {output_2}')\n",
        "print(f'Output 3 : {output_3}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_"
      },
      "source": [
        "**Inference without random key**\n",
        "\n",
        "The module that we built is inherently non-stochastic. In that case, passing a random key to the apply method seems redundant. Haiku offers another transformation `hk.without_apply_rng` which can be further wrapped around our `hk.transform` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output without random key in forward pass \n",
            " [[2.6736789 2.6259897]]\n"
          ]
        }
      ],
      "source": [
        "forward_without_rng = hk.without_apply_rng(hk.transform(_forward_fn_linear1))\n",
        "params = forward_without_rng.init(rng=rng_key, x=sample_x)\n",
        "output = forward_without_rng.apply(x=sample_x, params=params)\n",
        "print(f'Output without random key in forward pass \\n {output_1}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_"
      },
      "source": [
        "We can also mutate the parameters and then do forward inference to generate a different output for the same inputs. This is what is done to apply gradient descent to our parameters while learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mutated params \n",
            " : FlatMapping({\n",
            "  'my_linear1': FlatMapping({\n",
            "                  'b': DeviceArray([2., 2.], dtype=float32),\n",
            "                  'w': DeviceArray([[0.69649637, 1.5123801 ],\n",
            "                                    [1.0800915 , 0.6836995 ],\n",
            "                                    [1.6056666 , 1.5820701 ]], dtype=float32),\n",
            "                }),\n",
            "})\n",
            "Output with mutated params \n",
            " [[9.673679 9.62599 ]]\n"
          ]
        }
      ],
      "source": [
        "mutated_params = jax.tree_util.tree_map(lambda x: x+1., params)\n",
        "print(f'Mutated params \\n : {mutated_params}')\n",
        "mutated_output = forward_without_rng.apply(x=sample_x, params=mutated_params)\n",
        "print(f'Output with mutated params \\n {mutated_output}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_"
      },
      "source": [
        "### Stateful Inference in Haiku\n",
        "\n",
        "For some modules you might want to maintain and carry over the internal state across function calls. Here, we demonstrate a simple example, where we declare a state variable `counter` within our Haiku transformation which gets updated on each call to the function. Note that we didn't explicitly instantiate this as a Haiku module (the same could be replicated as a hk module as shown earlier).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial params:\n",
            "FlatMapping({\n",
            "  '~': FlatMapping({'multiplier': DeviceArray([1.], dtype=float32)}),\n",
            "})\n",
            "Initial state:\n",
            "FlatMapping({'~': FlatMapping({'counter': DeviceArray(1, dtype=int32)})})\n",
            "##########\n",
            "After 1 iterations:\n",
            "Output: [[6.]]\n",
            "State: FlatMapping({'~': FlatMapping({'counter': DeviceArray(2, dtype=int32)})})\n",
            "##########\n",
            "After 2 iterations:\n",
            "Output: [[7.]]\n",
            "State: FlatMapping({'~': FlatMapping({'counter': DeviceArray(3, dtype=int32)})})\n",
            "##########\n",
            "After 3 iterations:\n",
            "Output: [[8.]]\n",
            "State: FlatMapping({'~': FlatMapping({'counter': DeviceArray(4, dtype=int32)})})\n",
            "##########\n"
          ]
        }
      ],
      "source": [
        "def stateful_f(x):\n",
        "  counter = hk.get_state(\"counter\", shape=[], dtype=jnp.int32, init=jnp.ones)\n",
        "  multiplier = hk.get_parameter('multiplier', shape=[1,], dtype=x.dtype, init=jnp.ones)\n",
        "  hk.set_state(\"counter\", counter + 1)\n",
        "  output = x + multiplier * counter\n",
        "  return output\n",
        "\n",
        "stateful_forward = hk.without_apply_rng(hk.transform_with_state(stateful_f))\n",
        "sample_x = jnp.array([[5., ]])\n",
        "params, state = stateful_forward.init(x=sample_x, rng=rng_key)\n",
        "print(f'Initial params:\\n{params}\\nInitial state:\\n{state}')\n",
        "print('##########')\n",
        "for i in range(3):\n",
        "  output, state = stateful_forward.apply(params, state, x=sample_x)\n",
        "  print(f'After {i+1} iterations:\\nOutput: {output}\\nState: {state}')\n",
        "  print('##########')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_"
      },
      "source": [
        "### Built-in Haiku nets and nested modules\n",
        "\n",
        "The usual networks we use such as MLP, Convnets etc. are [defined already in Haiku](https://dm-haiku.readthedocs.io/en/latest/api.html#common-modules) and we can compose those modules to construct our custom Haiku Module.\n",
        "\n",
        "Look at the params dictionary to see how the params are nested in the same way as the modules are nested within our custom Haiku module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FlatMapping({\n",
              "  'custom_linear/~/hk_internal_linear/~/linear_0': FlatMapping({\n",
              "                                                     'w': DeviceArray([[ 1.51595   , -0.23353337]], dtype=float32),\n",
              "                                                     'b': DeviceArray([0., 0.], dtype=float32),\n",
              "                                                   }),\n",
              "  'custom_linear/~/hk_internal_linear/~/linear_1': FlatMapping({\n",
              "                                                     'w': DeviceArray([[-0.22075887, -0.27375957,  0.5931483 ],\n",
              "                                                                       [ 0.7818068 ,  0.72626334, -0.6860752 ]], dtype=float32),\n",
              "                                                     'b': DeviceArray([0., 0., 0.], dtype=float32),\n",
              "                                                   }),\n",
              "  'custom_linear/~/old_linear': FlatMapping({\n",
              "                                  'w': DeviceArray([[ 0.28584382,  0.31626168],\n",
              "                                                    [ 0.2335775 , -0.4827032 ],\n",
              "                                                    [-0.14647584, -0.7185701 ]], dtype=float32),\n",
              "                                  'b': DeviceArray([1., 1.], dtype=float32),\n",
              "                                }),\n",
              "})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# See: https://dm-haiku.readthedocs.io/en/latest/api.html#common-modules\n",
        "\n",
        "class MyModuleCustom(hk.Module):\n",
        "  def __init__(self, output_size=2, name='custom_linear'):\n",
        "    super().__init__(name=name)\n",
        "    self._internal_linear_1 = hk.nets.MLP(output_sizes=[2, 3], name='hk_internal_linear')\n",
        "    self._internal_linear_2 = MyLinear1(output_size=output_size, name='old_linear')\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self._internal_linear_2(self._internal_linear_1(x))\n",
        "\n",
        "def _custom_forward_fn(x):\n",
        "  module = MyModuleCustom()\n",
        "  return module(x)\n",
        "\n",
        "custom_forward_without_rng = hk.without_apply_rng(hk.transform(_custom_forward_fn))\n",
        "params = custom_forward_without_rng.init(rng=rng_key, x=sample_x)\n",
        "params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_"
      },
      "source": [
        "### Rng Keys with hk.next_rng_key()\n",
        "\n",
        "The modules that we saw earlier were all non-stochastic. Below we show how to sample random numbers to do stochastic inference.\n",
        "\n",
        "Haiku offers a trivial model for working with random numbers. Within a transformed function, `hk.next_rng_key()` returns a unique rng key.\n",
        "These unique keys are deterministically derived from an initial random key passed into the top-level transformed function, and are thus safe to use with JAX program transformations. \n",
        "\n",
        "Let us define a simple haiku function where we generate two random samples. Note that the next_rng_keys are determined from the initial random key passed to the apply method of the top-level transformed function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bernoullis are  : (DeviceArray(True, dtype=bool), DeviceArray(False, dtype=bool))\n",
            "\n",
            " Iteration 1\n",
            "Bernoullis are  : (DeviceArray(True, dtype=bool), DeviceArray(False, dtype=bool))\n",
            "\n",
            " Iteration 2\n",
            "Bernoullis are  : (DeviceArray(True, dtype=bool), DeviceArray(False, dtype=bool))\n",
            "\n",
            " Iteration 3\n",
            "Bernoullis are  : (DeviceArray(True, dtype=bool), DeviceArray(False, dtype=bool))\n",
            "\n",
            " Iteration 4\n",
            "Bernoullis are  : (DeviceArray(True, dtype=bool), DeviceArray(False, dtype=bool))\n",
            "\n",
            " Iteration 5\n",
            "Bernoullis are  : (DeviceArray(True, dtype=bool), DeviceArray(False, dtype=bool))\n"
          ]
        }
      ],
      "source": [
        "class HkRandom2(hk.Module):\n",
        "  def __init__(self, rate=0.5):\n",
        "    super().__init__()\n",
        "    self.rate = rate\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    key1 = hk.next_rng_key()\n",
        "    return jax.random.bernoulli(key1, 1.0 - self.rate, shape=x.shape)\n",
        "\n",
        "\n",
        "class HkRandomNest(hk.Module):\n",
        "  def __init__(self, rate=0.5):\n",
        "    super().__init__()\n",
        "    self.rate = rate\n",
        "    self._another_random_module = HkRandom2()\n",
        "\n",
        "  def __call__(self, x):\n",
        "    key2 = hk.next_rng_key()\n",
        "    p1 = self._another_random_module(x)\n",
        "    p2 = jax.random.bernoulli(key2, 1.0 - self.rate, shape=x.shape)\n",
        "    print(f'Bernoullis are  : {p1, p2}')\n",
        "\n",
        "# Note that the modules that are stochastic cannot be wrapped with hk.without_apply_rng()\n",
        "forward = hk.transform(lambda x: HkRandomNest()(x))\n",
        "\n",
        "x = jnp.array(1.)\n",
        "params = forward.init(rng_key, x=x)\n",
        "for i in range(5):\n",
        "  print(f'\\n Iteration {i+1}')\n",
        "  prediction = forward.apply(params, x=x, rng=rng_key)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
